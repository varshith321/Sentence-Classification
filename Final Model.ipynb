{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Basic Mathematical Operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#deep learning\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout, Activation, GlobalMaxPool1D\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional,Flatten,LSTM\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('train1.json')\n",
    "test = pd.read_json('test1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>although the internet as level topology has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>an as node can represent a wide variety of org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AIMX</td>\n",
       "      <td>in this paper we introduce a radically new app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OWNX</td>\n",
       "      <td>we successfully classify number number percent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OWNX</td>\n",
       "      <td>we release to the community the as level topol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0  MISC  although the internet as level topology has be...\n",
       "1  MISC  an as node can represent a wide variety of org...\n",
       "2  AIMX  in this paper we introduce a radically new app...\n",
       "3  OWNX  we successfully classify number number percent...\n",
       "4  OWNX  we release to the community the as level topol..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conating two dataframes \n",
    "frames = [df, test]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2864, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3564, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MISC                   1622\n",
       "OWNX                    807\n",
       "AIMX                    179\n",
       "CONT                    166\n",
       "BASE                     60\n",
       "MISC--the                 6\n",
       "AIMX--on                  4\n",
       "MISC--in                  4\n",
       "CONT--these               2\n",
       "OWNX                      2\n",
       "OWNX--after               2\n",
       "OWNX--we                  2\n",
       "MISC--several             2\n",
       "MISC--specifically,       2\n",
       "MISC--for                 2\n",
       "MISC--on                  2\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are duplicate values like 'MISC--the' which is similar to 'MISC' in the labels\n",
    "#replacing all the duplicate values\n",
    "\n",
    "replacements = {\n",
    "   'label': {\n",
    "       \n",
    "     \n",
    "      r'MISC--the': 'MISC',\n",
    "      r'MISC--specifically': 'MISC',\n",
    "      r'MISC--for': 'MISC',\n",
    "      r'MISC,': 'MISC',\n",
    "      r'MISC--on': 'MISC',\n",
    "      r'MISC--several': 'MISC',\n",
    "      r'MISC--in': 'MISC',\n",
    "      r'OWNX--we': 'OWNX',\n",
    "      r'OWNX--after': 'OWNX',\n",
    "      r'CONT--these': 'CONT',\n",
    "      r'AIMX--on': 'AIMX',       \n",
    "      \n",
    "   }\n",
    "}\n",
    "\n",
    "data.replace(replacements, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use label encoding for converting the labels to numeric values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['label'])\n",
    "data['label'] = le.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2129\n",
       "4     967\n",
       "0     215\n",
       "2     183\n",
       "1      67\n",
       "5       3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'although the internet as level topology has been extensively studied over the past few years little is known about the details of the as taxonomy '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dealing with the text data\n",
    "data['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet as well as for modeling its topology and growth '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The text is totally in English so we dont have to convert anything \n",
    "data['text'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the length of characters \n",
    "count = data['text'].apply(lambda x : len(str(x).split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x241c77b4e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x241c77a9eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and Lemmatizing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatizing is the process of converting every word into its root state\n",
    "For Example =  writing: write\n",
    "Stemming can also be done but lemmatizing converts the word to its root state where stemming only removes the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'although the internet as level topology has been extensively studied over the past few years little is known about the details of the as taxonomy '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I am using WhiteSpace Tokenizer along with the WorldNet Lemmatizer in nltk\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w, 'v') for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "data['text'] = data.text.apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stem = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'as',\n",
       " 'level',\n",
       " 'topology',\n",
       " 'have',\n",
       " 'be',\n",
       " 'extensively',\n",
       " 'study',\n",
       " 'over',\n",
       " 'the',\n",
       " 'past',\n",
       " 'few',\n",
       " 'years',\n",
       " 'little',\n",
       " 'be',\n",
       " 'know',\n",
       " 'about',\n",
       " 'the',\n",
       " 'detail',\n",
       " 'of',\n",
       " 'the',\n",
       " 'as',\n",
       " 'taxonomy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop word removal\n",
    "#Here I am using the stop words proposed in the given data.\n",
    "\n",
    "stop = ['of','a','and','the','in','to','for','that','is','on','are','with','as',\n",
    "        'by','be','an','which','it','from','or','can','have','these','has','such','we']\n",
    "\n",
    "#removes all the stop words\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this paper introduce radically new approach base machine learn techniques map all ases internet into natural taxonomy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text'].copy()\n",
    "y = data['label'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning or Deep learning models will not accept the text data directly so we have to convert it into numeric\n",
    "or vectors. There are some techniques like TFIDF, Glove Vector, Raw Character label encoding, Fast Text vectors by Facebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF model gives the inverse logarthmic weight to every word \n",
    "tfidf = TfidfVectorizer(stop_words=stop, max_features=800000)\n",
    "#Only training data need to fit through the TFIDF because by doing fit the model learns and it is applied while transform\n",
    "tfidf.fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (2213, 0)\t0.47536796671459186\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0:,:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 13)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.txt' #input file for glove embeddings\n",
    "word2vec_output_file = 'word2vec.txt' #input file for word2vec\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt' #pretrained GloVe Model\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec' #Saving word2vec file\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Keras built in tokenizer for word embeddings\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150 #Max number of features in the sequence\n",
    "X_train_encoded = t.texts_to_sequences(X_train) #converts text into sequence using keras text tokenizer\n",
    "X_test_encoded = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning models the sequences should be of same length, so for making it to same length the sequences should be padded to a certain length. By using the pad_sequences it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(X_train_encoded, maxlen=maxlen)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen=maxlen) #pad sequences to a maxlen of 150 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=5, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=67, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First lets try the Basic ML algorithm\n",
    "#Linear Support Vector Classifier is used here\n",
    "#TFIDF preprocessed sparce vectors are used here\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=67, C= 5,multi_class = 'ovr')\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3, 2, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 4, 4, 4,\n",
       "       3, 4, 0, 3, 4, 1, 3, 3, 4, 4, 0, 3, 2, 4, 4, 4, 4, 3, 4, 3, 4, 3,\n",
       "       0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3,\n",
       "       4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3,\n",
       "       3, 3, 3, 3, 2, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3,\n",
       "       3, 0, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 2,\n",
       "       4, 3, 0, 4, 2, 3, 0, 3, 3, 3, 3, 3, 4, 4, 4, 3, 2, 3, 4, 3, 4, 4,\n",
       "       3, 4, 3, 0, 3, 2, 4, 3, 3, 3, 3, 4, 3, 3, 4, 0, 3, 3, 0, 0, 3, 3,\n",
       "       2, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 1, 3, 4, 2, 3, 4,\n",
       "       4, 4, 3, 3, 4, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 0, 3, 3, 4, 3, 2, 4,\n",
       "       3, 1, 3, 4, 3, 3, 4, 0, 4, 3, 0, 4, 3, 3, 3, 0, 3, 3, 0, 2, 3, 3,\n",
       "       3, 4, 3, 4, 3, 3, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 0, 3, 3, 4, 3, 3,\n",
       "       3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, 3, 4, 4, 0, 3, 4, 3, 0, 3, 3, 3,\n",
       "       4, 4, 4, 4, 4, 3, 0, 3, 2, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3,\n",
       "       0, 3, 4, 1, 3, 4, 3, 4, 3, 0, 2, 3, 0, 4, 3, 3, 4, 3, 3, 0, 4, 3,\n",
       "       4, 3, 3, 3, 3, 0, 4, 4, 3, 3, 0, 3, 0, 3, 3, 4, 4, 4, 3, 3, 3, 3,\n",
       "       2, 3, 3, 4, 0, 3, 4, 1, 0, 4, 3, 3, 3, 3, 4, 3, 0, 0, 4, 3, 3, 3,\n",
       "       3, 4, 3, 4, 3, 2, 4, 3, 3, 0, 4, 4, 0, 3, 4, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 0, 3, 4, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 4, 3,\n",
       "       0, 4, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 4, 3, 3, 3, 3, 3, 3, 4, 2, 3,\n",
       "       3, 3, 3, 3, 0, 4, 3, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3,\n",
       "       4, 4, 0, 4, 4, 3, 1, 3, 4, 3, 3, 3, 2, 3, 3, 3, 3, 1, 3, 3, 4, 3,\n",
       "       4, 3, 2, 2, 3, 3, 0, 1, 0, 3, 4, 2, 3, 3, 3, 4, 4, 3, 3, 3, 4, 3,\n",
       "       3, 3, 4, 3, 4, 0, 0, 3, 2, 0, 3, 3, 3, 4, 3, 4, 3, 3, 3, 1, 3, 3,\n",
       "       3, 3, 3, 0, 4, 4, 4, 3, 1, 4, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 4,\n",
       "       3, 3, 3, 4, 4, 3, 2, 3, 3, 3, 3, 3, 4, 0, 3, 4, 3, 1, 3, 2, 3, 4,\n",
       "       3, 3, 3, 3, 2, 3, 3, 3, 3, 4, 3, 1, 4, 3, 4, 3, 4, 3, 3, 3, 0, 3,\n",
       "       3, 3, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 4, 4, 3,\n",
       "       4, 3, 4, 3, 3, 4, 3, 3, 0, 2, 2, 4, 4, 3, 3, 4, 4, 0, 4, 3, 3, 3,\n",
       "       3, 4, 3, 4, 0, 1, 0, 4, 0, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3,\n",
       "       3, 3, 2, 3, 3, 3, 3, 4, 3, 2, 4, 3, 0, 0, 3, 4, 3, 3, 3, 4, 3, 3,\n",
       "       3, 2, 3, 4, 3, 4, 4, 3, 3, 4, 3, 3, 3, 2, 3, 4, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 1, 3, 3, 4, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8541374474053296"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with the basic Support Vector Classifier the accuracy is around 85%. The accuracy can be increased by using deep learning algorithms and by using Ensembls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Deep learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Embedding keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as the sequential model in keras doesnt support Sparce matrix so we are converting it to array\n",
    "X_train_tfidf_array = X_train_tfidf.toarray()\n",
    "X_test_tfidf_array = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 2138 samples, validate on 713 samples\n",
      "Epoch 1/20\n",
      "2138/2138 [==============================] - 22s 11ms/step - loss: 2.6778 - acc: 0.5529 - val_loss: 2.2275 - val_acc: 0.6311\n",
      "Epoch 2/20\n",
      "2138/2138 [==============================] - 23s 11ms/step - loss: 2.4445 - acc: 0.5940 - val_loss: 2.2280 - val_acc: 0.6311\n",
      "Epoch 3/20\n",
      "2138/2138 [==============================] - 24s 11ms/step - loss: 1.9992 - acc: 0.5800 - val_loss: 1.5054 - val_acc: 0.6311\n",
      "Epoch 4/20\n",
      "2138/2138 [==============================] - 24s 11ms/step - loss: 1.1832 - acc: 0.5978 - val_loss: 1.0029 - val_acc: 0.6311\n",
      "Epoch 5/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0791 - acc: 0.5978 - val_loss: 0.9846 - val_acc: 0.6311\n",
      "Epoch 6/20\n",
      "2138/2138 [==============================] - 24s 11ms/step - loss: 1.0779 - acc: 0.5978 - val_loss: 0.9824 - val_acc: 0.6311\n",
      "Epoch 7/20\n",
      "2138/2138 [==============================] - 23s 11ms/step - loss: 1.0721 - acc: 0.5978 - val_loss: 0.9802 - val_acc: 0.6311\n",
      "Epoch 8/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0692 - acc: 0.5978 - val_loss: 0.9846 - val_acc: 0.6311\n",
      "Epoch 9/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0658 - acc: 0.5978 - val_loss: 0.9852 - val_acc: 0.6311\n",
      "Epoch 10/20\n",
      "2138/2138 [==============================] - 25s 12ms/step - loss: 1.0658 - acc: 0.5978 - val_loss: 0.9825 - val_acc: 0.6311\n",
      "Epoch 11/20\n",
      "2138/2138 [==============================] - 23s 11ms/step - loss: 1.0665 - acc: 0.5978 - val_loss: 0.9794 - val_acc: 0.6311\n",
      "Epoch 12/20\n",
      "2138/2138 [==============================] - 25s 12ms/step - loss: 1.0648 - acc: 0.5978 - val_loss: 0.9770 - val_acc: 0.6311\n",
      "Epoch 13/20\n",
      "2138/2138 [==============================] - 23s 11ms/step - loss: 1.0657 - acc: 0.5978 - val_loss: 0.9813 - val_acc: 0.6311\n",
      "Epoch 14/20\n",
      "2138/2138 [==============================] - 23s 11ms/step - loss: 1.0630 - acc: 0.5978 - val_loss: 0.9809 - val_acc: 0.6311\n",
      "Epoch 15/20\n",
      "2138/2138 [==============================] - 22s 11ms/step - loss: 1.0654 - acc: 0.5978 - val_loss: 0.9973 - val_acc: 0.6311\n",
      "Epoch 16/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0689 - acc: 0.5978 - val_loss: 1.0038 - val_acc: 0.6311\n",
      "Epoch 17/20\n",
      "2138/2138 [==============================] - 23s 11ms/step - loss: 1.0677 - acc: 0.5978 - val_loss: 0.9798 - val_acc: 0.6311\n",
      "Epoch 18/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0732 - acc: 0.5978 - val_loss: 0.9867 - val_acc: 0.6311\n",
      "Epoch 19/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0646 - acc: 0.5978 - val_loss: 0.9803 - val_acc: 0.6311\n",
      "Epoch 20/20\n",
      "2138/2138 [==============================] - 22s 10ms/step - loss: 1.0685 - acc: 0.5978 - val_loss: 0.9796 - val_acc: 0.6311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241db932b38>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 3263 #max len of the elements in the array\n",
    "num_class = 6    #total number of categorical values\n",
    "\n",
    "inputs = Input(shape=(MAX_LENGTH, ))  #input layer \n",
    "embedding_layer = Embedding(vocab_size,128,input_length=MAX_LENGTH)(inputs) #Embedding layer \n",
    "x = Flatten()(embedding_layer)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "model = Model(inputs=[inputs], outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(X_train_tfidf_array, batch_size=64, y=to_categorical(y_train), verbose=1, validation_split=0.25, \n",
    "          shuffle=True, epochs=20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The accuracy with the basic Machine Learning and Deep learning models\n",
    "1. Linear SVC : 85\n",
    "2. Embedding layer: 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.fit(X_train_tfidf_array, batch_size=64, y=to_categorical(y_train), verbose=1, validation_split=0.25, \\n          shuffle=True, epochs=5)\\n          '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(MAX_LENGTH, ))\n",
    "embedding_layer = Embedding(vocab_size,128,input_length=MAX_LENGTH)(inputs)\n",
    "\n",
    "x = LSTM(64)(embedding_layer)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "model = Model(inputs=[inputs], outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "'''\n",
    "model.fit(X_train_tfidf_array, batch_size=64, y=to_categorical(y_train), verbose=1, validation_split=0.25, \n",
    "          shuffle=True, epochs=5)\n",
    "          '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code is using embedding layer along with LSTM, due to complexity in the layers it is taking a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ensembling I will start with simple ensemble models like Adaboost, Gradient boost, Extra features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Stacked Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.83 (+/- 0.01) [Random Forest]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84 (+/- 0.01) [LinearSVC]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82 (+/- 0.01) [GB]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83 (+/- 0.01) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "clf1 =  KNeighborsClassifier(n_neighbors=1) #K Nearest Neighbours in Sklearn\n",
    "clf2 =  RandomForestClassifier(random_state=42) #Random forest Classifier\n",
    "#clf3 = MultinomialNB()  #Gaussian Naive Bayes\n",
    "clf3 =  LinearSVC(random_state=67, C= 5,multi_class = 'ovr') #Linear SVC\n",
    "clf4 =  GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "lr = LogisticRegression() #Logestic Regression\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "for clf, label in zip([clf1, clf2, clf3,clf4, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'LinearSVC',\n",
    "                       'GB',\n",
    "                       'StackingClassifier']):\n",
    "    scores = model_selection.cross_val_score(clf, X_train_tfidf, y_train, \n",
    "                                              cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6479663394109397"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "bdt.fit(X_train_tfidf, y_train)\n",
    "bdt.predict(X_test_tfidf)\n",
    "bdt.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.672306\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's auc: 0.771608\n",
      "[3]\tvalid_0's auc: 0.781501\n",
      "[4]\tvalid_0's auc: 0.787331\n",
      "[5]\tvalid_0's auc: 0.813492\n",
      "[6]\tvalid_0's auc: 0.81727\n",
      "[7]\tvalid_0's auc: 0.809578\n",
      "[8]\tvalid_0's auc: 0.808885\n",
      "[9]\tvalid_0's auc: 0.812554\n",
      "[10]\tvalid_0's auc: 0.813451\n",
      "[11]\tvalid_0's auc: 0.817121\n",
      "[12]\tvalid_0's auc: 0.813329\n",
      "[13]\tvalid_0's auc: 0.817542\n",
      "[14]\tvalid_0's auc: 0.825696\n",
      "[15]\tvalid_0's auc: 0.828169\n",
      "[16]\tvalid_0's auc: 0.831784\n",
      "[17]\tvalid_0's auc: 0.829501\n",
      "[18]\tvalid_0's auc: 0.832572\n",
      "[19]\tvalid_0's auc: 0.831784\n",
      "[20]\tvalid_0's auc: 0.833659\n",
      "[21]\tvalid_0's auc: 0.834448\n",
      "[22]\tvalid_0's auc: 0.837302\n",
      "[23]\tvalid_0's auc: 0.843009\n",
      "[24]\tvalid_0's auc: 0.843743\n",
      "[25]\tvalid_0's auc: 0.842194\n",
      "[26]\tvalid_0's auc: 0.843037\n",
      "[27]\tvalid_0's auc: 0.842901\n",
      "[28]\tvalid_0's auc: 0.84108\n",
      "[29]\tvalid_0's auc: 0.840781\n",
      "[30]\tvalid_0's auc: 0.84108\n",
      "[31]\tvalid_0's auc: 0.840291\n",
      "[32]\tvalid_0's auc: 0.840454\n",
      "[33]\tvalid_0's auc: 0.840291\n",
      "[34]\tvalid_0's auc: 0.843444\n",
      "[35]\tvalid_0's auc: 0.842547\n",
      "[36]\tvalid_0's auc: 0.842248\n",
      "[37]\tvalid_0's auc: 0.843444\n",
      "[38]\tvalid_0's auc: 0.842982\n",
      "[39]\tvalid_0's auc: 0.844803\n",
      "[40]\tvalid_0's auc: 0.842656\n",
      "[41]\tvalid_0's auc: 0.8432\n",
      "[42]\tvalid_0's auc: 0.84271\n",
      "[43]\tvalid_0's auc: 0.84532\n",
      "[44]\tvalid_0's auc: 0.84551\n",
      "[45]\tvalid_0's auc: 0.845456\n",
      "[46]\tvalid_0's auc: 0.847141\n",
      "[47]\tvalid_0's auc: 0.848445\n",
      "[48]\tvalid_0's auc: 0.852087\n",
      "[49]\tvalid_0's auc: 0.853854\n",
      "[50]\tvalid_0's auc: 0.852359\n",
      "[51]\tvalid_0's auc: 0.851381\n",
      "[52]\tvalid_0's auc: 0.852332\n",
      "[53]\tvalid_0's auc: 0.855349\n",
      "[54]\tvalid_0's auc: 0.857741\n",
      "[55]\tvalid_0's auc: 0.856355\n",
      "[56]\tvalid_0's auc: 0.855784\n",
      "[57]\tvalid_0's auc: 0.856355\n",
      "[58]\tvalid_0's auc: 0.856518\n",
      "[59]\tvalid_0's auc: 0.857686\n",
      "[60]\tvalid_0's auc: 0.858067\n",
      "[61]\tvalid_0's auc: 0.858665\n",
      "[62]\tvalid_0's auc: 0.858393\n",
      "[63]\tvalid_0's auc: 0.857333\n",
      "[64]\tvalid_0's auc: 0.857578\n",
      "[65]\tvalid_0's auc: 0.858611\n",
      "[66]\tvalid_0's auc: 0.858991\n",
      "[67]\tvalid_0's auc: 0.859562\n",
      "[68]\tvalid_0's auc: 0.858719\n",
      "[69]\tvalid_0's auc: 0.859643\n",
      "[70]\tvalid_0's auc: 0.859127\n",
      "[71]\tvalid_0's auc: 0.856817\n",
      "[72]\tvalid_0's auc: 0.855186\n",
      "[73]\tvalid_0's auc: 0.855485\n",
      "[74]\tvalid_0's auc: 0.855485\n",
      "[75]\tvalid_0's auc: 0.855104\n",
      "[76]\tvalid_0's auc: 0.854207\n",
      "[77]\tvalid_0's auc: 0.854072\n",
      "[78]\tvalid_0's auc: 0.853963\n",
      "[79]\tvalid_0's auc: 0.853392\n",
      "[80]\tvalid_0's auc: 0.853365\n",
      "[81]\tvalid_0's auc: 0.853854\n",
      "[82]\tvalid_0's auc: 0.853582\n",
      "[83]\tvalid_0's auc: 0.855485\n",
      "[84]\tvalid_0's auc: 0.855458\n",
      "[85]\tvalid_0's auc: 0.856518\n",
      "[86]\tvalid_0's auc: 0.856545\n",
      "[87]\tvalid_0's auc: 0.857034\n",
      "[88]\tvalid_0's auc: 0.856953\n",
      "[89]\tvalid_0's auc: 0.858013\n",
      "[90]\tvalid_0's auc: 0.857659\n",
      "[91]\tvalid_0's auc: 0.857224\n",
      "[92]\tvalid_0's auc: 0.858447\n",
      "[93]\tvalid_0's auc: 0.857768\n",
      "[94]\tvalid_0's auc: 0.857632\n",
      "[95]\tvalid_0's auc: 0.857034\n",
      "[96]\tvalid_0's auc: 0.855893\n",
      "[97]\tvalid_0's auc: 0.856518\n",
      "[98]\tvalid_0's auc: 0.856273\n",
      "[99]\tvalid_0's auc: 0.855648\n",
      "[100]\tvalid_0's auc: 0.854887\n",
      "[101]\tvalid_0's auc: 0.855594\n",
      "[102]\tvalid_0's auc: 0.856953\n",
      "[103]\tvalid_0's auc: 0.857795\n",
      "[104]\tvalid_0's auc: 0.857605\n",
      "[105]\tvalid_0's auc: 0.857795\n",
      "[106]\tvalid_0's auc: 0.858257\n",
      "[107]\tvalid_0's auc: 0.85804\n",
      "[108]\tvalid_0's auc: 0.858746\n",
      "[109]\tvalid_0's auc: 0.858121\n",
      "[110]\tvalid_0's auc: 0.857795\n",
      "[111]\tvalid_0's auc: 0.857877\n",
      "[112]\tvalid_0's auc: 0.85679\n",
      "[113]\tvalid_0's auc: 0.857496\n",
      "[114]\tvalid_0's auc: 0.858529\n",
      "[115]\tvalid_0's auc: 0.85842\n",
      "[116]\tvalid_0's auc: 0.859127\n",
      "[117]\tvalid_0's auc: 0.859752\n",
      "[118]\tvalid_0's auc: 0.859915\n",
      "[119]\tvalid_0's auc: 0.860024\n",
      "[120]\tvalid_0's auc: 0.86016\n",
      "[121]\tvalid_0's auc: 0.861329\n",
      "[122]\tvalid_0's auc: 0.861546\n",
      "[123]\tvalid_0's auc: 0.863041\n",
      "[124]\tvalid_0's auc: 0.863748\n",
      "[125]\tvalid_0's auc: 0.86527\n",
      "[126]\tvalid_0's auc: 0.865949\n",
      "[127]\tvalid_0's auc: 0.866982\n",
      "[128]\tvalid_0's auc: 0.867227\n",
      "[129]\tvalid_0's auc: 0.867689\n",
      "[130]\tvalid_0's auc: 0.868341\n",
      "[131]\tvalid_0's auc: 0.868531\n",
      "[132]\tvalid_0's auc: 0.868314\n",
      "[133]\tvalid_0's auc: 0.869673\n",
      "[134]\tvalid_0's auc: 0.869129\n",
      "[135]\tvalid_0's auc: 0.869129\n",
      "[136]\tvalid_0's auc: 0.869211\n",
      "[137]\tvalid_0's auc: 0.869945\n",
      "[138]\tvalid_0's auc: 0.869673\n",
      "[139]\tvalid_0's auc: 0.869591\n",
      "[140]\tvalid_0's auc: 0.870026\n",
      "[141]\tvalid_0's auc: 0.869537\n",
      "[142]\tvalid_0's auc: 0.870026\n",
      "[143]\tvalid_0's auc: 0.870624\n",
      "[144]\tvalid_0's auc: 0.870298\n",
      "[145]\tvalid_0's auc: 0.869917\n",
      "[146]\tvalid_0's auc: 0.869591\n",
      "[147]\tvalid_0's auc: 0.870461\n",
      "[148]\tvalid_0's auc: 0.870108\n",
      "[149]\tvalid_0's auc: 0.87076\n",
      "[150]\tvalid_0's auc: 0.870977\n",
      "[151]\tvalid_0's auc: 0.87057\n",
      "[152]\tvalid_0's auc: 0.870461\n",
      "[153]\tvalid_0's auc: 0.869646\n",
      "[154]\tvalid_0's auc: 0.869184\n",
      "[155]\tvalid_0's auc: 0.869455\n",
      "[156]\tvalid_0's auc: 0.869754\n",
      "[157]\tvalid_0's auc: 0.869401\n",
      "[158]\tvalid_0's auc: 0.868749\n",
      "[159]\tvalid_0's auc: 0.869347\n",
      "[160]\tvalid_0's auc: 0.869075\n",
      "[161]\tvalid_0's auc: 0.869564\n",
      "[162]\tvalid_0's auc: 0.871113\n",
      "[163]\tvalid_0's auc: 0.871168\n",
      "[164]\tvalid_0's auc: 0.871983\n",
      "[165]\tvalid_0's auc: 0.871521\n",
      "[166]\tvalid_0's auc: 0.872309\n",
      "[167]\tvalid_0's auc: 0.873559\n",
      "[168]\tvalid_0's auc: 0.873587\n",
      "[169]\tvalid_0's auc: 0.873559\n",
      "[170]\tvalid_0's auc: 0.874619\n",
      "[171]\tvalid_0's auc: 0.874837\n",
      "[172]\tvalid_0's auc: 0.875082\n",
      "[173]\tvalid_0's auc: 0.875435\n",
      "[174]\tvalid_0's auc: 0.876359\n",
      "[175]\tvalid_0's auc: 0.876359\n",
      "[176]\tvalid_0's auc: 0.87674\n",
      "[177]\tvalid_0's auc: 0.877609\n",
      "[178]\tvalid_0's auc: 0.877908\n",
      "[179]\tvalid_0's auc: 0.878343\n",
      "[180]\tvalid_0's auc: 0.878696\n",
      "[181]\tvalid_0's auc: 0.878968\n",
      "[182]\tvalid_0's auc: 0.878561\n",
      "[183]\tvalid_0's auc: 0.87818\n",
      "[184]\tvalid_0's auc: 0.878533\n",
      "[185]\tvalid_0's auc: 0.87924\n",
      "[186]\tvalid_0's auc: 0.880083\n",
      "[187]\tvalid_0's auc: 0.879729\n",
      "[188]\tvalid_0's auc: 0.879865\n",
      "[189]\tvalid_0's auc: 0.880028\n",
      "[190]\tvalid_0's auc: 0.880545\n",
      "[191]\tvalid_0's auc: 0.880382\n",
      "[192]\tvalid_0's auc: 0.880273\n",
      "[193]\tvalid_0's auc: 0.87992\n",
      "[194]\tvalid_0's auc: 0.879756\n",
      "[195]\tvalid_0's auc: 0.879947\n",
      "[196]\tvalid_0's auc: 0.880219\n",
      "[197]\tvalid_0's auc: 0.879539\n",
      "[198]\tvalid_0's auc: 0.879457\n",
      "[199]\tvalid_0's auc: 0.880055\n",
      "[200]\tvalid_0's auc: 0.879593\n",
      "[201]\tvalid_0's auc: 0.879974\n",
      "[202]\tvalid_0's auc: 0.880219\n",
      "[203]\tvalid_0's auc: 0.881007\n",
      "[204]\tvalid_0's auc: 0.881632\n",
      "[205]\tvalid_0's auc: 0.882801\n",
      "[206]\tvalid_0's auc: 0.883426\n",
      "[207]\tvalid_0's auc: 0.884051\n",
      "[208]\tvalid_0's auc: 0.884513\n",
      "[209]\tvalid_0's auc: 0.884866\n",
      "[210]\tvalid_0's auc: 0.88628\n",
      "[211]\tvalid_0's auc: 0.886551\n",
      "[212]\tvalid_0's auc: 0.886905\n",
      "[213]\tvalid_0's auc: 0.887149\n",
      "[214]\tvalid_0's auc: 0.887503\n",
      "[215]\tvalid_0's auc: 0.887639\n",
      "[216]\tvalid_0's auc: 0.887802\n",
      "[217]\tvalid_0's auc: 0.888699\n",
      "[218]\tvalid_0's auc: 0.889297\n",
      "[219]\tvalid_0's auc: 0.890139\n",
      "[220]\tvalid_0's auc: 0.890737\n",
      "[221]\tvalid_0's auc: 0.891281\n",
      "[222]\tvalid_0's auc: 0.891118\n",
      "[223]\tvalid_0's auc: 0.891743\n",
      "[224]\tvalid_0's auc: 0.891743\n",
      "[225]\tvalid_0's auc: 0.892477\n",
      "[226]\tvalid_0's auc: 0.891933\n",
      "[227]\tvalid_0's auc: 0.891743\n",
      "[228]\tvalid_0's auc: 0.891417\n",
      "[229]\tvalid_0's auc: 0.891661\n",
      "[230]\tvalid_0's auc: 0.892314\n",
      "[231]\tvalid_0's auc: 0.891879\n",
      "[232]\tvalid_0's auc: 0.892205\n",
      "[233]\tvalid_0's auc: 0.891851\n",
      "[234]\tvalid_0's auc: 0.891634\n",
      "[235]\tvalid_0's auc: 0.892042\n",
      "[236]\tvalid_0's auc: 0.892123\n",
      "[237]\tvalid_0's auc: 0.892477\n",
      "[238]\tvalid_0's auc: 0.892531\n",
      "[239]\tvalid_0's auc: 0.892613\n",
      "[240]\tvalid_0's auc: 0.89264\n",
      "[241]\tvalid_0's auc: 0.891933\n",
      "[242]\tvalid_0's auc: 0.891824\n",
      "[243]\tvalid_0's auc: 0.891553\n",
      "[244]\tvalid_0's auc: 0.891199\n",
      "[245]\tvalid_0's auc: 0.891688\n",
      "[246]\tvalid_0's auc: 0.89158\n",
      "[247]\tvalid_0's auc: 0.891389\n",
      "[248]\tvalid_0's auc: 0.890791\n",
      "[249]\tvalid_0's auc: 0.890737\n",
      "[250]\tvalid_0's auc: 0.890357\n",
      "[251]\tvalid_0's auc: 0.890248\n",
      "[252]\tvalid_0's auc: 0.890737\n",
      "[253]\tvalid_0's auc: 0.890927\n",
      "[254]\tvalid_0's auc: 0.890846\n",
      "[255]\tvalid_0's auc: 0.89052\n",
      "[256]\tvalid_0's auc: 0.890166\n",
      "[257]\tvalid_0's auc: 0.889786\n",
      "[258]\tvalid_0's auc: 0.889867\n",
      "[259]\tvalid_0's auc: 0.889867\n",
      "[260]\tvalid_0's auc: 0.890112\n",
      "[261]\tvalid_0's auc: 0.890329\n",
      "[262]\tvalid_0's auc: 0.890139\n",
      "[263]\tvalid_0's auc: 0.88984\n",
      "[264]\tvalid_0's auc: 0.889759\n",
      "[265]\tvalid_0's auc: 0.889487\n",
      "[266]\tvalid_0's auc: 0.888753\n",
      "[267]\tvalid_0's auc: 0.888046\n",
      "[268]\tvalid_0's auc: 0.888128\n",
      "[269]\tvalid_0's auc: 0.888617\n",
      "[270]\tvalid_0's auc: 0.888128\n",
      "[271]\tvalid_0's auc: 0.887693\n",
      "[272]\tvalid_0's auc: 0.887693\n",
      "[273]\tvalid_0's auc: 0.887639\n",
      "[274]\tvalid_0's auc: 0.887448\n",
      "[275]\tvalid_0's auc: 0.887041\n",
      "[276]\tvalid_0's auc: 0.887394\n",
      "[277]\tvalid_0's auc: 0.887231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278]\tvalid_0's auc: 0.887177\n",
      "[279]\tvalid_0's auc: 0.887503\n",
      "[280]\tvalid_0's auc: 0.887611\n",
      "[281]\tvalid_0's auc: 0.887503\n",
      "[282]\tvalid_0's auc: 0.888128\n",
      "[283]\tvalid_0's auc: 0.888101\n",
      "[284]\tvalid_0's auc: 0.888345\n",
      "[285]\tvalid_0's auc: 0.888508\n",
      "[286]\tvalid_0's auc: 0.888726\n",
      "[287]\tvalid_0's auc: 0.889215\n",
      "[288]\tvalid_0's auc: 0.889079\n",
      "[289]\tvalid_0's auc: 0.889351\n",
      "[290]\tvalid_0's auc: 0.889351\n",
      "[291]\tvalid_0's auc: 0.889867\n",
      "[292]\tvalid_0's auc: 0.890058\n",
      "[293]\tvalid_0's auc: 0.890492\n",
      "[294]\tvalid_0's auc: 0.891118\n",
      "[295]\tvalid_0's auc: 0.891063\n",
      "[296]\tvalid_0's auc: 0.89158\n",
      "[297]\tvalid_0's auc: 0.891634\n",
      "[298]\tvalid_0's auc: 0.891906\n",
      "[299]\tvalid_0's auc: 0.892232\n",
      "[300]\tvalid_0's auc: 0.892368\n",
      "[301]\tvalid_0's auc: 0.892531\n",
      "[302]\tvalid_0's auc: 0.892694\n",
      "[303]\tvalid_0's auc: 0.892776\n",
      "[304]\tvalid_0's auc: 0.893075\n",
      "[305]\tvalid_0's auc: 0.893509\n",
      "[306]\tvalid_0's auc: 0.893863\n",
      "[307]\tvalid_0's auc: 0.894026\n",
      "[308]\tvalid_0's auc: 0.894243\n",
      "[309]\tvalid_0's auc: 0.894651\n",
      "[310]\tvalid_0's auc: 0.894814\n",
      "[311]\tvalid_0's auc: 0.89476\n",
      "[312]\tvalid_0's auc: 0.895032\n",
      "[313]\tvalid_0's auc: 0.895602\n",
      "[314]\tvalid_0's auc: 0.895466\n",
      "[315]\tvalid_0's auc: 0.895738\n",
      "[316]\tvalid_0's auc: 0.895956\n",
      "[317]\tvalid_0's auc: 0.896309\n",
      "[318]\tvalid_0's auc: 0.896608\n",
      "[319]\tvalid_0's auc: 0.896255\n",
      "[320]\tvalid_0's auc: 0.896608\n",
      "[321]\tvalid_0's auc: 0.896853\n",
      "[322]\tvalid_0's auc: 0.896825\n",
      "[323]\tvalid_0's auc: 0.897124\n",
      "[324]\tvalid_0's auc: 0.897124\n",
      "[325]\tvalid_0's auc: 0.897451\n",
      "[326]\tvalid_0's auc: 0.897532\n",
      "[327]\tvalid_0's auc: 0.897505\n",
      "[328]\tvalid_0's auc: 0.896988\n",
      "[329]\tvalid_0's auc: 0.896907\n",
      "[330]\tvalid_0's auc: 0.897097\n",
      "[331]\tvalid_0's auc: 0.89707\n",
      "[332]\tvalid_0's auc: 0.897124\n",
      "[333]\tvalid_0's auc: 0.896798\n",
      "[334]\tvalid_0's auc: 0.896907\n",
      "[335]\tvalid_0's auc: 0.896934\n",
      "[336]\tvalid_0's auc: 0.897233\n",
      "[337]\tvalid_0's auc: 0.89726\n",
      "[338]\tvalid_0's auc: 0.897369\n",
      "[339]\tvalid_0's auc: 0.897369\n",
      "[340]\tvalid_0's auc: 0.897695\n",
      "[341]\tvalid_0's auc: 0.897451\n",
      "[342]\tvalid_0's auc: 0.89832\n",
      "[343]\tvalid_0's auc: 0.898048\n",
      "[344]\tvalid_0's auc: 0.898429\n",
      "[345]\tvalid_0's auc: 0.898429\n",
      "[346]\tvalid_0's auc: 0.898728\n",
      "[347]\tvalid_0's auc: 0.898973\n",
      "[348]\tvalid_0's auc: 0.898782\n",
      "[349]\tvalid_0's auc: 0.899109\n",
      "[350]\tvalid_0's auc: 0.899489\n",
      "[351]\tvalid_0's auc: 0.899272\n",
      "[352]\tvalid_0's auc: 0.899353\n",
      "[353]\tvalid_0's auc: 0.899\n",
      "[354]\tvalid_0's auc: 0.899081\n",
      "[355]\tvalid_0's auc: 0.898837\n",
      "[356]\tvalid_0's auc: 0.89987\n",
      "[357]\tvalid_0's auc: 0.900005\n",
      "[358]\tvalid_0's auc: 0.899897\n",
      "[359]\tvalid_0's auc: 0.899598\n",
      "[360]\tvalid_0's auc: 0.900033\n",
      "[361]\tvalid_0's auc: 0.899951\n",
      "[362]\tvalid_0's auc: 0.899571\n",
      "[363]\tvalid_0's auc: 0.899543\n",
      "[364]\tvalid_0's auc: 0.899326\n",
      "[365]\tvalid_0's auc: 0.899598\n",
      "[366]\tvalid_0's auc: 0.899706\n",
      "[367]\tvalid_0's auc: 0.899815\n",
      "[368]\tvalid_0's auc: 0.899815\n",
      "[369]\tvalid_0's auc: 0.899842\n",
      "[370]\tvalid_0's auc: 0.899951\n",
      "[371]\tvalid_0's auc: 0.900005\n",
      "[372]\tvalid_0's auc: 0.899924\n",
      "[373]\tvalid_0's auc: 0.90025\n",
      "[374]\tvalid_0's auc: 0.900277\n",
      "[375]\tvalid_0's auc: 0.899788\n",
      "[376]\tvalid_0's auc: 0.90006\n",
      "[377]\tvalid_0's auc: 0.899978\n",
      "[378]\tvalid_0's auc: 0.899788\n",
      "[379]\tvalid_0's auc: 0.899788\n",
      "[380]\tvalid_0's auc: 0.899815\n",
      "[381]\tvalid_0's auc: 0.899788\n",
      "[382]\tvalid_0's auc: 0.899978\n",
      "[383]\tvalid_0's auc: 0.900413\n",
      "[384]\tvalid_0's auc: 0.900603\n",
      "[385]\tvalid_0's auc: 0.900821\n",
      "[386]\tvalid_0's auc: 0.900495\n",
      "[387]\tvalid_0's auc: 0.900794\n",
      "[388]\tvalid_0's auc: 0.901011\n",
      "[389]\tvalid_0's auc: 0.900875\n",
      "[390]\tvalid_0's auc: 0.900875\n",
      "[391]\tvalid_0's auc: 0.90131\n",
      "[392]\tvalid_0's auc: 0.900957\n",
      "[393]\tvalid_0's auc: 0.90112\n",
      "[394]\tvalid_0's auc: 0.901201\n",
      "[395]\tvalid_0's auc: 0.901229\n",
      "[396]\tvalid_0's auc: 0.901201\n",
      "[397]\tvalid_0's auc: 0.901256\n",
      "[398]\tvalid_0's auc: 0.901174\n",
      "[399]\tvalid_0's auc: 0.900957\n",
      "[400]\tvalid_0's auc: 0.900902\n",
      "[401]\tvalid_0's auc: 0.901201\n",
      "[402]\tvalid_0's auc: 0.901528\n",
      "[403]\tvalid_0's auc: 0.901364\n",
      "[404]\tvalid_0's auc: 0.901364\n",
      "[405]\tvalid_0's auc: 0.901174\n",
      "[406]\tvalid_0's auc: 0.901283\n",
      "[407]\tvalid_0's auc: 0.901147\n",
      "[408]\tvalid_0's auc: 0.900902\n",
      "[409]\tvalid_0's auc: 0.900848\n",
      "[410]\tvalid_0's auc: 0.90131\n",
      "[411]\tvalid_0's auc: 0.901854\n",
      "[412]\tvalid_0's auc: 0.901636\n",
      "[413]\tvalid_0's auc: 0.901528\n",
      "[414]\tvalid_0's auc: 0.901609\n",
      "[415]\tvalid_0's auc: 0.901582\n",
      "[416]\tvalid_0's auc: 0.901229\n",
      "[417]\tvalid_0's auc: 0.901256\n",
      "[418]\tvalid_0's auc: 0.901201\n",
      "[419]\tvalid_0's auc: 0.901392\n",
      "[420]\tvalid_0's auc: 0.90112\n",
      "[421]\tvalid_0's auc: 0.901256\n",
      "[422]\tvalid_0's auc: 0.901419\n",
      "[423]\tvalid_0's auc: 0.901337\n",
      "[424]\tvalid_0's auc: 0.901555\n",
      "[425]\tvalid_0's auc: 0.901636\n",
      "[426]\tvalid_0's auc: 0.901854\n",
      "[427]\tvalid_0's auc: 0.901718\n",
      "[428]\tvalid_0's auc: 0.901663\n",
      "[429]\tvalid_0's auc: 0.901935\n",
      "[430]\tvalid_0's auc: 0.901718\n",
      "[431]\tvalid_0's auc: 0.902261\n",
      "[432]\tvalid_0's auc: 0.902424\n",
      "[433]\tvalid_0's auc: 0.902479\n",
      "[434]\tvalid_0's auc: 0.902424\n",
      "[435]\tvalid_0's auc: 0.902343\n",
      "[436]\tvalid_0's auc: 0.90237\n",
      "[437]\tvalid_0's auc: 0.902805\n",
      "[438]\tvalid_0's auc: 0.902941\n",
      "[439]\tvalid_0's auc: 0.90305\n",
      "[440]\tvalid_0's auc: 0.902914\n",
      "[441]\tvalid_0's auc: 0.902723\n",
      "[442]\tvalid_0's auc: 0.902941\n",
      "[443]\tvalid_0's auc: 0.903349\n",
      "[444]\tvalid_0's auc: 0.903376\n",
      "[445]\tvalid_0's auc: 0.902832\n",
      "[446]\tvalid_0's auc: 0.902886\n",
      "[447]\tvalid_0's auc: 0.902859\n",
      "[448]\tvalid_0's auc: 0.902533\n",
      "[449]\tvalid_0's auc: 0.902506\n",
      "[450]\tvalid_0's auc: 0.902533\n",
      "[451]\tvalid_0's auc: 0.902751\n",
      "[452]\tvalid_0's auc: 0.902832\n",
      "[453]\tvalid_0's auc: 0.902696\n",
      "[454]\tvalid_0's auc: 0.902669\n",
      "[455]\tvalid_0's auc: 0.902778\n",
      "[456]\tvalid_0's auc: 0.902615\n",
      "[457]\tvalid_0's auc: 0.902805\n",
      "[458]\tvalid_0's auc: 0.902479\n",
      "[459]\tvalid_0's auc: 0.902669\n",
      "[460]\tvalid_0's auc: 0.902886\n",
      "[461]\tvalid_0's auc: 0.903213\n",
      "[462]\tvalid_0's auc: 0.903213\n",
      "[463]\tvalid_0's auc: 0.903158\n",
      "[464]\tvalid_0's auc: 0.90343\n",
      "[465]\tvalid_0's auc: 0.903457\n",
      "[466]\tvalid_0's auc: 0.90362\n",
      "[467]\tvalid_0's auc: 0.903566\n",
      "[468]\tvalid_0's auc: 0.903702\n",
      "[469]\tvalid_0's auc: 0.90411\n",
      "[470]\tvalid_0's auc: 0.903947\n",
      "[471]\tvalid_0's auc: 0.90362\n",
      "[472]\tvalid_0's auc: 0.903947\n",
      "[473]\tvalid_0's auc: 0.903892\n",
      "[474]\tvalid_0's auc: 0.904572\n",
      "[475]\tvalid_0's auc: 0.904517\n",
      "[476]\tvalid_0's auc: 0.904572\n",
      "[477]\tvalid_0's auc: 0.904572\n",
      "[478]\tvalid_0's auc: 0.904952\n",
      "[479]\tvalid_0's auc: 0.905034\n",
      "[480]\tvalid_0's auc: 0.905333\n",
      "[481]\tvalid_0's auc: 0.905251\n",
      "[482]\tvalid_0's auc: 0.905061\n",
      "[483]\tvalid_0's auc: 0.904871\n",
      "[484]\tvalid_0's auc: 0.904354\n",
      "[485]\tvalid_0's auc: 0.903865\n",
      "[486]\tvalid_0's auc: 0.90362\n",
      "[487]\tvalid_0's auc: 0.903512\n",
      "[488]\tvalid_0's auc: 0.903593\n",
      "[489]\tvalid_0's auc: 0.90324\n",
      "[490]\tvalid_0's auc: 0.903185\n",
      "[491]\tvalid_0's auc: 0.903104\n",
      "[492]\tvalid_0's auc: 0.902914\n",
      "[493]\tvalid_0's auc: 0.902506\n",
      "[494]\tvalid_0's auc: 0.902207\n",
      "[495]\tvalid_0's auc: 0.902316\n",
      "[496]\tvalid_0's auc: 0.902424\n",
      "[497]\tvalid_0's auc: 0.902234\n",
      "[498]\tvalid_0's auc: 0.902153\n",
      "[499]\tvalid_0's auc: 0.902125\n",
      "[500]\tvalid_0's auc: 0.902098\n",
      "[501]\tvalid_0's auc: 0.902234\n",
      "[502]\tvalid_0's auc: 0.90237\n",
      "[503]\tvalid_0's auc: 0.902343\n",
      "[504]\tvalid_0's auc: 0.902397\n",
      "[505]\tvalid_0's auc: 0.902289\n",
      "[506]\tvalid_0's auc: 0.902044\n",
      "[507]\tvalid_0's auc: 0.902017\n",
      "[508]\tvalid_0's auc: 0.902017\n",
      "[509]\tvalid_0's auc: 0.902397\n",
      "[510]\tvalid_0's auc: 0.90237\n",
      "[511]\tvalid_0's auc: 0.902044\n",
      "[512]\tvalid_0's auc: 0.902125\n",
      "[513]\tvalid_0's auc: 0.901799\n",
      "[514]\tvalid_0's auc: 0.901691\n",
      "[515]\tvalid_0's auc: 0.9015\n",
      "[516]\tvalid_0's auc: 0.901283\n",
      "[517]\tvalid_0's auc: 0.901364\n",
      "[518]\tvalid_0's auc: 0.901174\n",
      "[519]\tvalid_0's auc: 0.90131\n",
      "[520]\tvalid_0's auc: 0.901283\n",
      "[521]\tvalid_0's auc: 0.90131\n",
      "[522]\tvalid_0's auc: 0.901555\n",
      "[523]\tvalid_0's auc: 0.901772\n",
      "[524]\tvalid_0's auc: 0.901799\n",
      "[525]\tvalid_0's auc: 0.902071\n",
      "[526]\tvalid_0's auc: 0.902071\n",
      "[527]\tvalid_0's auc: 0.902289\n",
      "[528]\tvalid_0's auc: 0.902696\n",
      "[529]\tvalid_0's auc: 0.902832\n",
      "[530]\tvalid_0's auc: 0.902941\n",
      "[531]\tvalid_0's auc: 0.902886\n",
      "[532]\tvalid_0's auc: 0.90256\n",
      "[533]\tvalid_0's auc: 0.902533\n",
      "[534]\tvalid_0's auc: 0.902452\n",
      "[535]\tvalid_0's auc: 0.902506\n",
      "[536]\tvalid_0's auc: 0.902642\n",
      "[537]\tvalid_0's auc: 0.902723\n",
      "[538]\tvalid_0's auc: 0.902615\n",
      "[539]\tvalid_0's auc: 0.902805\n",
      "[540]\tvalid_0's auc: 0.902995\n",
      "[541]\tvalid_0's auc: 0.902968\n",
      "[542]\tvalid_0's auc: 0.903022\n",
      "[543]\tvalid_0's auc: 0.903077\n",
      "[544]\tvalid_0's auc: 0.903213\n",
      "[545]\tvalid_0's auc: 0.903185\n",
      "[546]\tvalid_0's auc: 0.903213\n",
      "[547]\tvalid_0's auc: 0.903077\n",
      "[548]\tvalid_0's auc: 0.903077\n",
      "[549]\tvalid_0's auc: 0.90324\n",
      "[550]\tvalid_0's auc: 0.903349\n",
      "[551]\tvalid_0's auc: 0.903185\n",
      "[552]\tvalid_0's auc: 0.903484\n",
      "[553]\tvalid_0's auc: 0.903512\n",
      "[554]\tvalid_0's auc: 0.903566\n",
      "[555]\tvalid_0's auc: 0.903484\n",
      "[556]\tvalid_0's auc: 0.903512\n",
      "[557]\tvalid_0's auc: 0.903539\n",
      "[558]\tvalid_0's auc: 0.903403\n",
      "[559]\tvalid_0's auc: 0.903593\n",
      "[560]\tvalid_0's auc: 0.903539\n",
      "[561]\tvalid_0's auc: 0.904001\n",
      "[562]\tvalid_0's auc: 0.904191\n",
      "[563]\tvalid_0's auc: 0.904517\n",
      "[564]\tvalid_0's auc: 0.905034\n",
      "[565]\tvalid_0's auc: 0.905224\n",
      "[566]\tvalid_0's auc: 0.905197\n",
      "[567]\tvalid_0's auc: 0.905061\n",
      "[568]\tvalid_0's auc: 0.905414\n",
      "[569]\tvalid_0's auc: 0.905496\n",
      "[570]\tvalid_0's auc: 0.905903\n",
      "[571]\tvalid_0's auc: 0.905985\n",
      "[572]\tvalid_0's auc: 0.906284\n",
      "[573]\tvalid_0's auc: 0.906692\n",
      "[574]\tvalid_0's auc: 0.9068\n",
      "[575]\tvalid_0's auc: 0.907127\n",
      "[576]\tvalid_0's auc: 0.907616\n",
      "[577]\tvalid_0's auc: 0.908023\n",
      "[578]\tvalid_0's auc: 0.908431\n",
      "[579]\tvalid_0's auc: 0.908567\n",
      "[580]\tvalid_0's auc: 0.909029\n",
      "[581]\tvalid_0's auc: 0.908431\n",
      "[582]\tvalid_0's auc: 0.908431\n",
      "[583]\tvalid_0's auc: 0.908486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[584]\tvalid_0's auc: 0.908322\n",
      "[585]\tvalid_0's auc: 0.908295\n",
      "[586]\tvalid_0's auc: 0.907942\n",
      "[587]\tvalid_0's auc: 0.907833\n",
      "[588]\tvalid_0's auc: 0.907643\n",
      "[589]\tvalid_0's auc: 0.907371\n",
      "[590]\tvalid_0's auc: 0.907371\n",
      "[591]\tvalid_0's auc: 0.907072\n",
      "[592]\tvalid_0's auc: 0.906991\n",
      "[593]\tvalid_0's auc: 0.906637\n",
      "[594]\tvalid_0's auc: 0.906447\n",
      "[595]\tvalid_0's auc: 0.906284\n",
      "[596]\tvalid_0's auc: 0.906012\n",
      "[597]\tvalid_0's auc: 0.905577\n",
      "[598]\tvalid_0's auc: 0.905604\n",
      "[599]\tvalid_0's auc: 0.905523\n",
      "[600]\tvalid_0's auc: 0.905142\n",
      "[601]\tvalid_0's auc: 0.905686\n",
      "[602]\tvalid_0's auc: 0.905876\n",
      "[603]\tvalid_0's auc: 0.906583\n",
      "[604]\tvalid_0's auc: 0.906882\n",
      "[605]\tvalid_0's auc: 0.907697\n",
      "[606]\tvalid_0's auc: 0.907996\n",
      "[607]\tvalid_0's auc: 0.908513\n",
      "[608]\tvalid_0's auc: 0.908948\n",
      "[609]\tvalid_0's auc: 0.908866\n",
      "[610]\tvalid_0's auc: 0.909274\n",
      "[611]\tvalid_0's auc: 0.909165\n",
      "[612]\tvalid_0's auc: 0.90941\n",
      "[613]\tvalid_0's auc: 0.908893\n",
      "[614]\tvalid_0's auc: 0.908812\n",
      "[615]\tvalid_0's auc: 0.909247\n",
      "[616]\tvalid_0's auc: 0.909573\n",
      "[617]\tvalid_0's auc: 0.909627\n",
      "[618]\tvalid_0's auc: 0.909736\n",
      "[619]\tvalid_0's auc: 0.909709\n",
      "[620]\tvalid_0's auc: 0.909681\n",
      "[621]\tvalid_0's auc: 0.909546\n",
      "[622]\tvalid_0's auc: 0.909627\n",
      "[623]\tvalid_0's auc: 0.909219\n",
      "[624]\tvalid_0's auc: 0.909328\n",
      "[625]\tvalid_0's auc: 0.909165\n",
      "[626]\tvalid_0's auc: 0.909138\n",
      "[627]\tvalid_0's auc: 0.908866\n",
      "[628]\tvalid_0's auc: 0.908975\n",
      "[629]\tvalid_0's auc: 0.909029\n",
      "[630]\tvalid_0's auc: 0.908567\n",
      "[631]\tvalid_0's auc: 0.90835\n",
      "[632]\tvalid_0's auc: 0.908132\n",
      "[633]\tvalid_0's auc: 0.907942\n",
      "[634]\tvalid_0's auc: 0.907616\n",
      "[635]\tvalid_0's auc: 0.907453\n",
      "[636]\tvalid_0's auc: 0.907371\n",
      "[637]\tvalid_0's auc: 0.907235\n",
      "[638]\tvalid_0's auc: 0.907045\n",
      "[639]\tvalid_0's auc: 0.906773\n",
      "[640]\tvalid_0's auc: 0.906447\n",
      "[641]\tvalid_0's auc: 0.906882\n",
      "[642]\tvalid_0's auc: 0.907045\n",
      "[643]\tvalid_0's auc: 0.907072\n",
      "[644]\tvalid_0's auc: 0.907317\n",
      "[645]\tvalid_0's auc: 0.907643\n",
      "[646]\tvalid_0's auc: 0.908023\n",
      "[647]\tvalid_0's auc: 0.908132\n",
      "[648]\tvalid_0's auc: 0.908051\n",
      "[649]\tvalid_0's auc: 0.908159\n",
      "[650]\tvalid_0's auc: 0.908078\n",
      "[651]\tvalid_0's auc: 0.908241\n",
      "[652]\tvalid_0's auc: 0.90873\n",
      "[653]\tvalid_0's auc: 0.908812\n",
      "[654]\tvalid_0's auc: 0.909219\n",
      "[655]\tvalid_0's auc: 0.908975\n",
      "[656]\tvalid_0's auc: 0.909138\n",
      "[657]\tvalid_0's auc: 0.909165\n",
      "[658]\tvalid_0's auc: 0.909056\n",
      "[659]\tvalid_0's auc: 0.90941\n",
      "[660]\tvalid_0's auc: 0.909219\n",
      "[661]\tvalid_0's auc: 0.909464\n",
      "[662]\tvalid_0's auc: 0.910008\n",
      "[663]\tvalid_0's auc: 0.910008\n",
      "[664]\tvalid_0's auc: 0.910116\n",
      "[665]\tvalid_0's auc: 0.910252\n",
      "[666]\tvalid_0's auc: 0.910388\n",
      "[667]\tvalid_0's auc: 0.910252\n",
      "[668]\tvalid_0's auc: 0.910361\n",
      "[669]\tvalid_0's auc: 0.910796\n",
      "[670]\tvalid_0's auc: 0.91085\n",
      "[671]\tvalid_0's auc: 0.911204\n",
      "[672]\tvalid_0's auc: 0.911312\n",
      "[673]\tvalid_0's auc: 0.911448\n",
      "[674]\tvalid_0's auc: 0.91153\n",
      "[675]\tvalid_0's auc: 0.911557\n",
      "[676]\tvalid_0's auc: 0.911584\n",
      "[677]\tvalid_0's auc: 0.911638\n",
      "[678]\tvalid_0's auc: 0.911666\n",
      "[679]\tvalid_0's auc: 0.911693\n",
      "[680]\tvalid_0's auc: 0.911693\n",
      "[681]\tvalid_0's auc: 0.911421\n",
      "[682]\tvalid_0's auc: 0.911503\n",
      "[683]\tvalid_0's auc: 0.911339\n",
      "[684]\tvalid_0's auc: 0.911394\n",
      "[685]\tvalid_0's auc: 0.911611\n",
      "[686]\tvalid_0's auc: 0.911448\n",
      "[687]\tvalid_0's auc: 0.91172\n",
      "[688]\tvalid_0's auc: 0.911149\n",
      "[689]\tvalid_0's auc: 0.911367\n",
      "[690]\tvalid_0's auc: 0.910986\n",
      "[691]\tvalid_0's auc: 0.910905\n",
      "[692]\tvalid_0's auc: 0.91066\n",
      "[693]\tvalid_0's auc: 0.910796\n",
      "[694]\tvalid_0's auc: 0.910687\n",
      "[695]\tvalid_0's auc: 0.910687\n",
      "[696]\tvalid_0's auc: 0.910796\n",
      "[697]\tvalid_0's auc: 0.910932\n",
      "[698]\tvalid_0's auc: 0.910932\n",
      "[699]\tvalid_0's auc: 0.91085\n",
      "[700]\tvalid_0's auc: 0.910986\n",
      "[701]\tvalid_0's auc: 0.910796\n",
      "[702]\tvalid_0's auc: 0.910741\n",
      "[703]\tvalid_0's auc: 0.910796\n",
      "[704]\tvalid_0's auc: 0.910959\n",
      "[705]\tvalid_0's auc: 0.910769\n",
      "[706]\tvalid_0's auc: 0.910714\n",
      "[707]\tvalid_0's auc: 0.910578\n",
      "[708]\tvalid_0's auc: 0.910497\n",
      "[709]\tvalid_0's auc: 0.910606\n",
      "[710]\tvalid_0's auc: 0.910714\n",
      "[711]\tvalid_0's auc: 0.910578\n",
      "[712]\tvalid_0's auc: 0.91047\n",
      "[713]\tvalid_0's auc: 0.910497\n",
      "[714]\tvalid_0's auc: 0.910361\n",
      "[715]\tvalid_0's auc: 0.910442\n",
      "[716]\tvalid_0's auc: 0.910442\n",
      "[717]\tvalid_0's auc: 0.910388\n",
      "[718]\tvalid_0's auc: 0.910225\n",
      "[719]\tvalid_0's auc: 0.910116\n",
      "[720]\tvalid_0's auc: 0.910089\n",
      "[721]\tvalid_0's auc: 0.910089\n",
      "[722]\tvalid_0's auc: 0.90998\n",
      "[723]\tvalid_0's auc: 0.910035\n",
      "[724]\tvalid_0's auc: 0.909953\n",
      "[725]\tvalid_0's auc: 0.909763\n",
      "[726]\tvalid_0's auc: 0.909437\n",
      "[727]\tvalid_0's auc: 0.909491\n",
      "[728]\tvalid_0's auc: 0.909518\n",
      "[729]\tvalid_0's auc: 0.909355\n",
      "[730]\tvalid_0's auc: 0.909328\n",
      "[731]\tvalid_0's auc: 0.909002\n",
      "[732]\tvalid_0's auc: 0.908703\n",
      "[733]\tvalid_0's auc: 0.908458\n",
      "[734]\tvalid_0's auc: 0.908132\n",
      "[735]\tvalid_0's auc: 0.908105\n",
      "[736]\tvalid_0's auc: 0.908241\n",
      "[737]\tvalid_0's auc: 0.908105\n",
      "[738]\tvalid_0's auc: 0.908023\n",
      "[739]\tvalid_0's auc: 0.907942\n",
      "[740]\tvalid_0's auc: 0.90835\n",
      "[741]\tvalid_0's auc: 0.907833\n",
      "[742]\tvalid_0's auc: 0.907942\n",
      "[743]\tvalid_0's auc: 0.907725\n",
      "[744]\tvalid_0's auc: 0.907725\n",
      "[745]\tvalid_0's auc: 0.907996\n",
      "[746]\tvalid_0's auc: 0.90748\n",
      "[747]\tvalid_0's auc: 0.90767\n",
      "[748]\tvalid_0's auc: 0.90748\n",
      "[749]\tvalid_0's auc: 0.907398\n",
      "[750]\tvalid_0's auc: 0.907181\n",
      "[751]\tvalid_0's auc: 0.907317\n",
      "[752]\tvalid_0's auc: 0.907154\n",
      "[753]\tvalid_0's auc: 0.907018\n",
      "[754]\tvalid_0's auc: 0.906882\n",
      "[755]\tvalid_0's auc: 0.907018\n",
      "[756]\tvalid_0's auc: 0.906855\n",
      "[757]\tvalid_0's auc: 0.906828\n",
      "[758]\tvalid_0's auc: 0.906991\n",
      "[759]\tvalid_0's auc: 0.906692\n",
      "[760]\tvalid_0's auc: 0.906746\n",
      "[761]\tvalid_0's auc: 0.907072\n",
      "[762]\tvalid_0's auc: 0.906991\n",
      "[763]\tvalid_0's auc: 0.907235\n",
      "[764]\tvalid_0's auc: 0.907235\n",
      "[765]\tvalid_0's auc: 0.907045\n",
      "[766]\tvalid_0's auc: 0.90729\n",
      "[767]\tvalid_0's auc: 0.907643\n",
      "[768]\tvalid_0's auc: 0.907969\n",
      "[769]\tvalid_0's auc: 0.907752\n",
      "[770]\tvalid_0's auc: 0.90786\n",
      "[771]\tvalid_0's auc: 0.908431\n",
      "[772]\tvalid_0's auc: 0.908812\n",
      "[773]\tvalid_0's auc: 0.908703\n",
      "[774]\tvalid_0's auc: 0.908594\n",
      "[775]\tvalid_0's auc: 0.908649\n",
      "[776]\tvalid_0's auc: 0.90873\n",
      "[777]\tvalid_0's auc: 0.908703\n",
      "[778]\tvalid_0's auc: 0.908621\n",
      "[779]\tvalid_0's auc: 0.908785\n",
      "[780]\tvalid_0's auc: 0.909083\n",
      "[781]\tvalid_0's auc: 0.909192\n",
      "[782]\tvalid_0's auc: 0.909111\n",
      "[783]\tvalid_0's auc: 0.909111\n",
      "[784]\tvalid_0's auc: 0.909382\n",
      "[785]\tvalid_0's auc: 0.909491\n",
      "[786]\tvalid_0's auc: 0.909138\n",
      "[787]\tvalid_0's auc: 0.909382\n",
      "Early stopping, best iteration is:\n",
      "[687]\tvalid_0's auc: 0.91172\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "#For Light gbm the trainig data should in form of Dataset\n",
    "#Converting the X_train to the lgb Dataset\n",
    "train_data = lgb.Dataset(X_train_tfidf, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_tfidf, label=y_test)\n",
    "\n",
    "model = lgb.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets=test_data,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)\n",
    "y = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.74953402  3.29126042  3.14771065  2.34534736  3.13987713  3.07850488\n",
      "  2.31425661  2.96573642  2.97148446  2.26511346  2.68866328  2.75684235\n",
      "  2.98144417  3.15683004  2.77503991  3.06366156  2.96058965  3.47460267\n",
      "  1.34544329  3.6009856   3.89762438  3.74465329  3.32926776  3.95418001\n",
      "  1.46308046  2.09871987  3.72370511  2.35643472  2.94553256  2.86771996\n",
      "  3.90194749  4.01510893  0.07565199  3.01824617  2.93057589  2.38267478\n",
      "  3.42628882  4.05494693  2.55089139  2.93135859  3.95773359  2.9503273\n",
      "  3.82104972  2.92345155  1.25183337  2.93380266  3.06692387  2.95443086\n",
      "  3.01479051  3.06700929  3.04857808  3.23424693  2.83481234  3.00961421\n",
      "  2.93448883  3.43049562  3.07487976  2.88714701  3.52453006  3.17252758\n",
      "  2.86696662  3.98692234  2.90222934  2.91549017  2.92981271  3.32622525\n",
      "  3.82213316  3.50809254  3.49363674  3.02947528  3.41908682  3.36543407\n",
      "  2.77107834  2.97049226  2.98025328  2.8296077   2.77884915  2.41313374\n",
      "  3.76868538  2.99836634  2.94923839  3.38794529  3.9874582   2.97794057\n",
      "  3.10357086  2.67359745  3.17124972  2.85755517  3.17252758  3.01159567\n",
      "  2.93993749  3.01527096  2.26511346  2.48851552  2.16931085  4.00171365\n",
      "  2.74736798  3.79989596  2.90949505  3.02148022  3.07664994  2.8966586\n",
      "  2.59934905  2.65288114  3.24137522  2.85835151  3.15683004  3.90784729\n",
      "  3.94279415  2.98302272  3.06061588  1.16812314  3.01029981  3.46582966\n",
      "  3.40098293  3.14414686  3.3017422   2.59934905  3.56091507  3.06169214\n",
      "  3.71812265  2.95910105  2.56188186  2.98419679  2.97154407  2.71623687\n",
      "  3.35570183  3.70794519  2.9988715   3.22653227  3.12242883  2.03188774\n",
      "  3.90384217  2.9322722   1.46888821  3.71605733  2.41305901  3.14413217\n",
      "  0.11517344  3.13673779  2.78350028  2.98058048  2.99325458  3.12535936\n",
      "  3.43578529  3.45667388  3.82946341  2.77235825  1.9983647   2.9144669\n",
      "  4.00171365  3.14771065  2.92398359  2.50877095  3.26388664  3.54834685\n",
      "  2.89576899  1.87157322  2.58876069  2.21008443  3.47460267  3.10040827\n",
      "  2.80099971  3.09800005  3.17124972  3.71303015  2.94749356  2.85315932\n",
      "  3.71812265  1.45519487  3.08174148  2.72132634  1.98464255  0.08956336\n",
      "  2.85835151  3.15715189  2.53398592  2.90911735  3.09801754  2.6349537\n",
      "  2.76341392  3.07160049  2.64154268  3.86999945  3.03030269  2.99551639\n",
      "  3.55070533  3.1544658   2.66012977  3.96514192  3.41770899  2.09871987\n",
      "  2.02498382  3.39101443  3.98601687  2.58467669  2.90656702  4.0359192\n",
      "  3.35535206  3.10989367  2.76744705  2.60548762  3.4282281   2.99444011\n",
      "  3.10404005  3.32926776  2.13174794  3.0509323   3.00803011  2.15492941\n",
      "  3.21080877  2.6349537   2.74879977  0.51422403  3.72468089  3.09718695\n",
      "  4.00285082  2.93674982  2.86817445  3.63593443  3.52426734  1.35460054\n",
      "  2.96573642  2.7188133   3.19360725  3.04897888  2.51028784 -0.04938303\n",
      "  3.91543427  3.18206574  1.87157322  3.62650984  2.93175253  2.90354261\n",
      "  2.94332655  0.05969056  2.94006926  3.14008538  0.17446458  2.86817445\n",
      "  2.9271082   2.92345155  2.81516422  3.95418001  2.71072048  3.51093258\n",
      "  2.58560302  3.38535958  3.89807661  1.99946973  2.80513358  3.65577783\n",
      "  3.63114932  3.0334931   3.43325287  2.1941606   2.88714701  3.44000517\n",
      "  0.092821    2.73371316  3.00209183  3.71236294  2.90518763  2.980411\n",
      "  3.0336348   3.75662938  3.23224425  3.13484705  3.18635922  3.0334931\n",
      "  3.99888985  4.01414293  3.66970454  2.9371439   3.37792007  3.03006171\n",
      "  3.83123673  3.65154325  2.44413127  2.98338618  3.96841774  3.35255366\n",
      "  0.35794466  3.30708351  2.98620906  3.01062694  3.45571765  3.80696745\n",
      "  3.21702958  3.95773359  3.59460503  3.0017578   1.6620733   3.01470015\n",
      "  1.90243569  3.91594083  3.05576774  3.46697784  3.42933545  3.83148479\n",
      "  3.30580971  3.11115793  2.73195958  2.97075025  2.94572539  3.05694659\n",
      "  4.26167945  3.15779955  0.63985871  3.1854238   3.57300807  1.80405881\n",
      "  3.17332856  3.73654576  3.04053046  3.05101025  3.03836408  0.71330618\n",
      "  2.04011196  2.73034368 -0.08943964  3.566042    2.92304704  2.88465968\n",
      "  4.0277909   2.97991291  3.45702706  1.45519487  3.74672796  2.4903356\n",
      "  4.05593075  3.14641621  2.94572539  3.1544658   2.77915003  0.58277869\n",
      "  3.39730041  2.74360683  3.2206177   2.88465968  0.03107897  3.01594704\n",
      "  0.17674933  3.02879954  2.93819775  3.85912383  3.93147177  2.51028784\n",
      "  3.29581682  2.99836634  3.20015386  2.95205273  2.41772583  2.78518699\n",
      "  2.94298721  3.87106576  0.20616819  2.96486382  3.57806817  2.35643472\n",
      "  1.16812314  4.04209699  2.64833515  3.46582966  3.16191877  3.43049562\n",
      "  2.9012518   3.55463705  1.46308046  1.91674903  2.04526237  2.65578042\n",
      "  3.4640292   2.79515479  2.86152217  2.74360683  3.09656603  3.12600708\n",
      "  2.95628517  1.9983647   3.80026818  2.74879977  2.98315531  0.53900285\n",
      "  2.67359745  3.34869304  0.20530609  2.89381772  3.59359381  3.41446658\n",
      "  2.96139978  3.32554638  3.03982137  3.20317274  3.31950345  3.03140534\n",
      "  2.73953847  0.6206609   2.9322722   3.09640891  3.20795641  2.83540866\n",
      "  2.38295352  3.43584921  3.1312277   3.2206177   2.98764544  3.66238597\n",
      "  3.03595957  2.90973906  2.86696662  2.98593965  3.4282281   3.50954021\n",
      "  2.9503273   3.27426312  3.54834685  2.82342989  1.57057703  2.38267478\n",
      "  3.09801754  3.37792007  4.01133001  3.04533681  3.41335397  3.3099917\n",
      "  3.08505227  3.89527769  3.88118454  3.11439413  3.95773359  2.74736798\n",
      "  2.71072048  2.72822351  2.74681248  3.06692387  3.21080877  2.54029517\n",
      "  2.16931085  2.1941606   2.52079358  3.08280831  3.03581436  3.01217483\n",
      " -0.04938303  3.87106576  3.10971479  2.95205273 -0.04938303  2.98240644\n",
      "  3.17334074  3.39522514  2.8296077   2.94298721  3.24291733  1.87243362\n",
      "  3.04117644  3.74953402  3.49257235  2.94006926  2.95266297  2.94574902\n",
      "  3.71236294  3.70812883  1.17460175  3.66970821  3.90088473  2.58876069\n",
      "  1.35460054  3.1110178   3.85912383  3.11439413  3.00508307  3.05090544\n",
      "  1.97655876  3.0283324   3.04918366  3.70812883  3.05239176  1.41268754\n",
      "  3.27169682  2.9599046   2.67359745  3.08505227  1.75486579  3.35255366\n",
      "  2.38295352  3.00961421  3.00508307  2.9938085   2.14723744  0.98821329\n",
      "  0.51263692  2.84315752  3.36892958  2.93057589  2.76955683  2.87505916\n",
      "  2.57632573  3.81692044  3.75341416  2.93091216  2.99043367  2.96139978\n",
      "  3.93127055  3.15779955  3.15884904  3.01159567  4.25711124  3.10427879\n",
      "  3.29306289  1.45519487  0.57028078  2.89225155  2.28716583  0.4369376\n",
      "  2.97557715  3.04035025  3.07850488  3.39756284  2.91687472  3.83123673\n",
      "  2.99551639  2.35031325  2.93993749  1.17785139  2.74681375  2.91129888\n",
      "  2.84798239  2.85755517  3.32622525  2.14723744  3.64981334  3.90088473\n",
      "  3.39443886  3.03581436  1.35460054  3.85785042  3.99888985  3.04745087\n",
      "  3.20015386  3.63593443  3.0605919   3.10340987  3.15715189  3.49254872\n",
      "  2.42771947  3.58189342  2.77490965  3.97827524  2.74736798  3.23784886\n",
      "  3.08899579  3.25116582  3.70812883  2.92533021  2.5995016   2.95266297\n",
      "  3.36217732  2.92861123  2.64154268  2.99262441  3.14932976  1.12074701\n",
      "  2.92084139  4.0359192   3.03836408  1.04497075  3.11439413  2.21541484\n",
      "  2.78350028  3.48935203  3.0036678   3.04076817  3.70812883  3.49097734\n",
      "  2.01773032  3.45186718  2.05683622  3.01817859  3.49254872  3.21702958\n",
      "  3.01200301  1.34544329  4.01277954  3.55070533  3.65154325  3.39153214\n",
      "  4.07263176  2.73195958  3.01911542  3.05090544  0.61706345  3.40287073\n",
      "  2.74879977  2.99992975  3.90784729  3.39756284  3.17877811  2.97291307\n",
      "  2.97291307  2.65613926  3.01207294  3.15224473  3.38794529  2.95141106\n",
      "  2.94574902  3.94981029  3.40331031  3.6009856   2.97991291  3.01440669\n",
      "  3.73542284  3.95579386  3.51228918  3.48442383  3.87428651  2.7416621\n",
      "  2.85465271  2.9271082   2.93819775  3.45571765  3.39153214  2.76341392\n",
      "  0.19400766  1.9983647   2.86817445  3.60707718  3.19578463  3.11075848\n",
      "  3.15683004  3.4282281   3.52453006  0.23657539  3.69929804  2.99444011\n",
      "  3.04053046  2.94395646  3.07627558  3.59221759  3.09801754  3.62650984\n",
      "  0.71330618  0.98821329  0.71330618  3.37792007  0.71361804  3.67140448\n",
      "  3.49452759  3.03006171  3.70794519  2.95035075  2.89585     3.05090544\n",
      "  3.00820518  2.97964922  3.20317274  2.74980828  3.80439308  3.3017422\n",
      "  3.06181933  2.4903356   2.41772583  3.06654697  2.74894846  2.9938085\n",
      "  2.94619385  3.3099917   2.54187296  2.21008443  3.77440236  3.17460069\n",
      "  0.17977078  1.90016103  3.223538    2.34588993  2.90911735  2.97034185\n",
      "  2.93636146  4.19137803  3.01594704  2.980411    3.08593285  3.00961421\n",
      "  2.94542159  3.74840635  2.6349537   4.10866218  3.50932169  2.90436848\n",
      "  3.10427879  3.86999945  2.98315531  2.90222934  3.02879954  2.58467669\n",
      "  3.25969     3.48762407  3.04816962  3.1110178   2.94574902  3.26388664\n",
      "  2.89225155  2.87300153  2.93636146  3.11154146  2.02498382  3.02269849\n",
      "  3.33789468  2.89189751  2.98620906  2.97863036  3.08883742]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model Sofar with an validation accuracy of 91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8022440392706872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_tfidf, y_train)\n",
    "predictions = gbm.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "#xgb.Score(X_test_tfidf, y_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Glove Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the code is inspired from fellow kaggler from:\n",
    "# 'https://www.kaggle.com/demesgal/lstm-glove-lr-decrease-bn-cv-lb-0-047 '\n",
    "\n",
    "EMBEDDING_FILE='glove.6B.50d.txt'\n",
    "\n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 3330 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 150 # max number of words in a comment to use\n",
    "\n",
    "\n",
    "#using the pretrained glove word2vec embedded values \n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32') #reading the glove word in the vector form from pretrained model\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE,encoding=\"utf8\"))\n",
    "\n",
    "#By using these vectors we have to create the matrix \n",
    "\n",
    "all_embs = np.stack(embeddings_index.values()) #joining the sequence\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std() #Embedding Mean and Strandard deviation\n",
    "\n",
    "word_index = t.word_index\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM with half sized embedding with two fully connected layers\n",
    "\n",
    "inp = Input(shape=(maxlen,)) #input layer\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp) #Embedding layer\n",
    "x = Bidirectional(LSTM(50, return_sequences=True,dropout=0.1, recurrent_dropout=0.1))(x) #Bidirectional LSTM\n",
    "x = GlobalMaxPool1D()(x)  #MAX Pooling \n",
    "x = BatchNormalization()(x) #Batch Normalization\n",
    "x = Dense(50, activation=\"relu\")(x) \n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2851/2851 [==============================] - 23s 8ms/step - loss: 1.3358 - acc: 0.4904\n",
      "Epoch 2/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.9167 - acc: 0.6398\n",
      "Epoch 3/30\n",
      "2851/2851 [==============================] - 19s 7ms/step - loss: 0.7172 - acc: 0.7155\n",
      "Epoch 4/30\n",
      "2851/2851 [==============================] - 21s 7ms/step - loss: 0.5381 - acc: 0.7938\n",
      "Epoch 5/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.4124 - acc: 0.8502\n",
      "Epoch 6/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.3370 - acc: 0.8786\n",
      "Epoch 7/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.3014 - acc: 0.8839\n",
      "Epoch 8/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.2651 - acc: 0.8920\n",
      "Epoch 9/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.2479 - acc: 0.9000\n",
      "Epoch 10/30\n",
      "2851/2851 [==============================] - 16s 6ms/step - loss: 0.2277 - acc: 0.8986\n",
      "Epoch 11/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.2193 - acc: 0.8997\n",
      "Epoch 12/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.2167 - acc: 0.8965\n",
      "Epoch 13/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.1961 - acc: 0.9049\n",
      "Epoch 14/30\n",
      "2851/2851 [==============================] - 19s 7ms/step - loss: 0.1900 - acc: 0.9039\n",
      "Epoch 15/30\n",
      "2851/2851 [==============================] - 19s 7ms/step - loss: 0.1888 - acc: 0.9106\n",
      "Epoch 16/30\n",
      "2851/2851 [==============================] - 19s 7ms/step - loss: 0.1804 - acc: 0.9109\n",
      "Epoch 17/30\n",
      "2851/2851 [==============================] - 20s 7ms/step - loss: 0.1750 - acc: 0.9074\n",
      "Epoch 18/30\n",
      "2851/2851 [==============================] - 19s 7ms/step - loss: 0.1772 - acc: 0.9085\n",
      "Epoch 19/30\n",
      "2851/2851 [==============================] - 21s 7ms/step - loss: 0.1688 - acc: 0.9116\n",
      "Epoch 20/30\n",
      "2851/2851 [==============================] - 17s 6ms/step - loss: 0.1602 - acc: 0.9120\n",
      "Epoch 21/30\n",
      "2851/2851 [==============================] - 16s 6ms/step - loss: 0.1622 - acc: 0.9106\n",
      "Epoch 22/30\n",
      "2851/2851 [==============================] - 16s 6ms/step - loss: 0.1540 - acc: 0.9137\n",
      "Epoch 23/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1525 - acc: 0.9092\n",
      "Epoch 24/30\n",
      "2851/2851 [==============================] - 16s 6ms/step - loss: 0.1544 - acc: 0.9067\n",
      "Epoch 25/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1523 - acc: 0.9127\n",
      "Epoch 26/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1511 - acc: 0.9123\n",
      "Epoch 27/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1489 - acc: 0.9155\n",
      "Epoch 28/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1447 - acc: 0.9123\n",
      "Epoch 29/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1469 - acc: 0.9134\n",
      "Epoch 30/30\n",
      "2851/2851 [==============================] - 18s 6ms/step - loss: 0.1460 - acc: 0.9095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241de8b9c88>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train, batch_size=64, epochs=30) #,validation_data=(X_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bidirectional LSTM along with the Glove Embedding vectors gives an accuracy of 91%. This is the best accuracy so far followed by the LightBGM.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The accuracy of this proble is 91% by \n",
    "1). LightBGM\n",
    "2). Embedding+LSTM+GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
