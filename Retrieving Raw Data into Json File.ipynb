{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting all the list of files required for Sentence Classification only from the Labelled Dat\n",
    "\n",
    "def list_files_from_directory(directory):\n",
    "\n",
    "    ret_val = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            ret_val.append(str(directory) + \"/\" + str(file))\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used for Reading all the lines in the gien path\n",
    "#If we want to use the the 'Stop Words' suggested by them in the folder 'stopwords.txt', we can reterive using this function\n",
    "\n",
    "def read_file(path):\n",
    "\n",
    "    f = open(path, \"r\")\n",
    "    read = f.readlines()\n",
    "    ret_val = []\n",
    "    for line in read:\n",
    "        if line.startswith(\"#\"):\n",
    "            pass\n",
    "        else:\n",
    "            ret_val.append(line)\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used for seperating the sentences from their respective labels \n",
    "#The label is written first in CAPS followed by the sentence\n",
    "#returns labels with sentence\n",
    "\n",
    "def process_line(line):\n",
    "\n",
    "    if \"\\t\" in line:\n",
    "        splits = line.split(\"\\t\")\n",
    "        s_category = splits[0]         #categories\n",
    "        sentence = splits[1].lower()   #texts\n",
    "        #for sw in stopwords:\n",
    "            #sentence = sentence.replace(sw, \"\")\n",
    "        pattern = re.compile(\"[^\\w']\")\n",
    "        sentence = pattern.sub(' ', sentence)\n",
    "        sentence = re.sub(' +', ' ', sentence)\n",
    "        return s_category, sentence\n",
    "    else:\n",
    "        splits = line.split(\" \")\n",
    "        s_category = splits[0]\n",
    "        sentence = line[len(s_category)+1:].lower()\n",
    "        #for sw in stopwords:\n",
    "            #sentence = sentence.replace(sw, \"\")\n",
    "        pattern = re.compile(\"[^\\w']\")\n",
    "        sentence = pattern.sub(' ', sentence)\n",
    "        sentence = re.sub(' +', ' ', sentence)\n",
    "        return s_category, sentence\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# There are three types of labeled data namely \n",
    "1). ARXIV\n",
    "2). JDM\n",
    "3). PLOS\n",
    "\n",
    "There are 30 text files in each format and 25 of them are calssified as training data and rest five is used as the test data\n",
    "First 25 Training sets\n",
    "Last five as the Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The returned values from the above function is used \n",
    "#the values are appended in to a formatted json file \n",
    "#Train and Test data should be seperated for training and testing\n",
    "\n",
    "def create_json_file(input_folder, destination_file):\n",
    "\n",
    "    tr_folder = list_files_from_directory(input_folder)\n",
    "    all_json = []\n",
    "    for file in tr_folder:\n",
    "        lines = read_file(file)\n",
    "        for line in lines:\n",
    "            c, s = process_line(line)\n",
    "            if s.endswith('\\n'):\n",
    "                s = s[:-1]\n",
    "            json_data = {\n",
    "                'text': s,     #labeling as text and label\n",
    "                'label': c\n",
    "            }\n",
    "            all_json.append(json_data)  #converting to json file\n",
    "\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        json.dump(all_json, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(\"training_set\", \"train1.json\")\n",
    "create_json_file(\"test_set\", \"test1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
